{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0174fca",
   "metadata": {},
   "source": [
    "## 1. Load Corpus Data from JSON\n",
    "\n",
    "Load the Socratic corpus from the JSON files into Python data structures for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c0dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up paths\n",
    "PROJECT_ROOT = Path(\"/workspaces/constellation-chronicle\")\n",
    "DATA_DIR = PROJECT_ROOT / \"src\" / \"data\"\n",
    "CORPUS_DIR = DATA_DIR / \"corpus\"\n",
    "\n",
    "# Load corpus files\n",
    "def load_corpus():\n",
    "    \"\"\"Load the Socratic corpus from JSON and .me files\"\"\"\n",
    "    try:\n",
    "        # Load questions\n",
    "        with open(CORPUS_DIR / \"socratic_questions.json\", 'r', encoding='utf-8') as f:\n",
    "            questions = json.load(f)\n",
    "        \n",
    "        # Load critica text\n",
    "        with open(CORPUS_DIR / \"critica_socratica_lagrange.me\", 'r', encoding='utf-8') as f:\n",
    "            critica_text = f.read()\n",
    "        \n",
    "        print(f\"Loaded {len(questions)} questions and {len(critica_text)} characters of critica text\")\n",
    "        return questions, critica_text\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading corpus: {e}\")\n",
    "        return [], \"\"\n",
    "\n",
    "questions, critica_text = load_corpus()\n",
    "print(f\"Sample question: {questions[0] if questions else 'No questions loaded'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0e1aa8",
   "metadata": {},
   "source": [
    "## 2. Process Corpus with LLM\n",
    "\n",
    "Use an LLM to analyze and summarize the corpus, extracting key themes or dialogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3faa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock LLM processing (replace with actual API calls)\n",
    "def process_with_llm(text, questions):\n",
    "    \"\"\"Process corpus with LLM to extract themes and structure\"\"\"\n",
    "    # This would integrate with NotebookLM or similar\n",
    "    # For now, we'll simulate the analysis\n",
    "    \n",
    "    themes = []\n",
    "    dialogues = []\n",
    "    \n",
    "    # Extract themes from questions\n",
    "    for q in questions[:5]:  # Process first 5 questions\n",
    "        theme = {\n",
    "            'eje': q.get('eje', 'unknown'),\n",
    "            'tension': q.get('tension', 'unknown'),\n",
    "            'question': q.get('texto', ''),\n",
    "            'summary': f\"Analysis of {q.get('eje', 'unknown')} theme\"\n",
    "        }\n",
    "        themes.append(theme)\n",
    "    \n",
    "    # Extract dialogue fragments from critica text\n",
    "    lines = critica_text.split('\\n')[:10]  # First 10 lines\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip():\n",
    "            dialogues.append({\n",
    "                'speaker': 'S√≥crates' if i % 2 == 0 else 'Interlocutor',\n",
    "                'text': line.strip(),\n",
    "                'context': 'critica_socratica'\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        'themes': themes,\n",
    "        'dialogues': dialogues,\n",
    "        'summary': f\"Processed {len(themes)} themes and {len(dialogues)} dialogue fragments\"\n",
    "    }\n",
    "\n",
    "corpus_analysis = process_with_llm(critica_text, questions)\n",
    "print(\"LLM Processing Results:\")\n",
    "print(json.dumps(corpus_analysis, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99dc1c",
   "metadata": {},
   "source": [
    "## 3. Generate Structured Scripts\n",
    "\n",
    "Generate structured scripts from the processed corpus, organizing content into episodes or segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf2184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_scripts(analysis, num_episodes=3):\n",
    "    \"\"\"Generate structured episode scripts from corpus analysis\"\"\"\n",
    "    scripts = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        episode = {\n",
    "            'id': i + 1,\n",
    "            'title': f\"Episodio {i+1}: {analysis['themes'][i]['eje'].title()}\",\n",
    "            'description': f\"Exploraci√≥n del eje {analysis['themes'][i]['eje']} a trav√©s del di√°logo socr√°tico\",\n",
    "            'duration': \"25:00\",\n",
    "            'season': 1,\n",
    "            'script': {\n",
    "                'introduction': f\"Bienvenidos al episodio {i+1} del Sistema Lagrange. Hoy exploramos {analysis['themes'][i]['eje']}.\",\n",
    "                'main_content': [\n",
    "                    {\n",
    "                        'type': 'dialogue',\n",
    "                        'speaker': analysis['dialogues'][i*2]['speaker'] if i*2 < len(analysis['dialogues']) else 'Narrador',\n",
    "                        'text': analysis['dialogues'][i*2]['text'] if i*2 < len(analysis['dialogues']) else analysis['themes'][i]['question']\n",
    "                    },\n",
    "                    {\n",
    "                        'type': 'question',\n",
    "                        'text': analysis['themes'][i]['question']\n",
    "                    }\n",
    "                ],\n",
    "                'conclusion': \"Reflexionemos sobre estas preguntas en nuestro camino hacia la consciencia.\"\n",
    "            }\n",
    "        }\n",
    "        scripts.append(episode)\n",
    "    \n",
    "    return scripts\n",
    "\n",
    "episode_scripts = generate_episode_scripts(corpus_analysis)\n",
    "print(f\"Generated {len(episode_scripts)} episode scripts\")\n",
    "for script in episode_scripts:\n",
    "    print(f\"- {script['title']}: {len(script['script']['main_content'])} segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e18c4",
   "metadata": {},
   "source": [
    "## 4. Implement Text-to-Audio Pipeline\n",
    "\n",
    "Set up a pipeline to convert text scripts to audio files using text-to-speech libraries or APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c78212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-Audio Pipeline (simulated)\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "def text_to_audio_pipeline(scripts, output_dir=\"public/episodes\"):\n",
    "    \"\"\"Convert text scripts to audio files\"\"\"\n",
    "    audio_files = []\n",
    "    \n",
    "    # Create output directory\n",
    "    output_path = PROJECT_ROOT / output_dir\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    for script in scripts:\n",
    "        # Generate audio filename\n",
    "        audio_filename = f\"episode_{script['id']}.mp3\"\n",
    "        audio_path = output_path / audio_filename\n",
    "        \n",
    "        # Simulate TTS processing\n",
    "        print(f\"Converting script for {script['title']} to audio...\")\n",
    "        \n",
    "        # Combine all text from script\n",
    "        full_text = script['script']['introduction'] + \" \"\n",
    "        for segment in script['script']['main_content']:\n",
    "            if segment['type'] == 'dialogue':\n",
    "                full_text += f\"{segment['speaker']}: {segment['text']} \"\n",
    "            else:\n",
    "                full_text += segment['text'] + \" \"\n",
    "        full_text += script['script']['conclusion']\n",
    "        \n",
    "        # Simulate TTS API call (replace with actual TTS)\n",
    "        # For demo, we'll create a placeholder audio file\n",
    "        with open(audio_path, 'w') as f:\n",
    "            f.write(f\"# Simulated audio file for {script['title']}\\n\")\n",
    "            f.write(f\"Text length: {len(full_text)} characters\\n\")\n",
    "            f.write(f\"Duration: {script['duration']}\\n\")\n",
    "        \n",
    "        # Simulate processing time\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        audio_files.append({\n",
    "            'episode_id': script['id'],\n",
    "            'filename': audio_filename,\n",
    "            'path': str(audio_path),\n",
    "            'text_length': len(full_text),\n",
    "            'duration': script['duration']\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úì Generated {audio_filename}\")\n",
    "    \n",
    "    return audio_files\n",
    "\n",
    "audio_files = text_to_audio_pipeline(episode_scripts)\n",
    "print(f\"\\nGenerated {len(audio_files)} audio files:\")\n",
    "for audio in audio_files:\n",
    "    print(f\"- {audio['filename']}: {audio['duration']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7be55",
   "metadata": {},
   "source": [
    "## 5. Create Episodes JSON and Audio Files\n",
    "\n",
    "Compile episodes into a JSON structure and pair with generated audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9789b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_episodes_json(scripts, audio_files):\n",
    "    \"\"\"Create episodes.json with metadata and audio links\"\"\"\n",
    "    episodes = []\n",
    "    \n",
    "    for script, audio in zip(scripts, audio_files):\n",
    "        episode = {\n",
    "            'id': script['id'],\n",
    "            'title': script['title'],\n",
    "            'description': script['description'],\n",
    "            'duration': script['duration'],\n",
    "            'publishedAt': \"2024-12-17\",  # Current date\n",
    "            'audioUrl': f\"/episodes/{audio['filename']}\",\n",
    "            'chapterId': script['id'],  # Link to chapter\n",
    "            'season': script['season']\n",
    "        }\n",
    "        episodes.append(episode)\n",
    "    \n",
    "    # Save to episodes.json\n",
    "    episodes_path = DATA_DIR / \"podcast\" / \"episodes.json\"\n",
    "    with open(episodes_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(episodes, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Saved {len(episodes)} episodes to {episodes_path}\")\n",
    "    return episodes\n",
    "\n",
    "episodes_data = create_episodes_json(episode_scripts, audio_files)\n",
    "print(\"Episodes JSON structure:\")\n",
    "print(json.dumps(episodes_data[:1], indent=2, ensure_ascii=False))  # Show first episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57fe0dc",
   "metadata": {},
   "source": [
    "## 6. Integrate with Frontend SPA\n",
    "\n",
    "Prepare data for integration with a Single Page Application (SPA) frontend, such as exporting JSON for playback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_frontend_data(episodes, scripts):\n",
    "    \"\"\"Prepare data structures for frontend consumption\"\"\"\n",
    "    frontend_data = {\n",
    "        'episodes': episodes,\n",
    "        'scripts': {ep['id']: ep['script'] for ep in scripts},\n",
    "        'metadata': {\n",
    "            'total_episodes': len(episodes),\n",
    "            'last_updated': \"2024-12-17\",\n",
    "            'pipeline_version': \"1.0\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save frontend-ready data\n",
    "    frontend_path = PROJECT_ROOT / \"src\" / \"data\" / \"frontend_data.json\"\n",
    "    with open(frontend_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(frontend_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Frontend data prepared and saved to {frontend_path}\")\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = f\"\"\"\n",
    "Pipeline Execution Summary:\n",
    "==========================\n",
    "- Corpus loaded: {len(questions)} questions, {len(critica_text)} chars of text\n",
    "- Themes extracted: {len(corpus_analysis['themes'])}\n",
    "- Episodes generated: {len(episodes)}\n",
    "- Audio files created: {len(audio_files)}\n",
    "- Frontend data ready: ‚úì\n",
    "\n",
    "Next Steps:\n",
    "1. Replace mock LLM with actual NotebookLM integration\n",
    "2. Implement real TTS API (e.g., Google TTS, OpenAI TTS)\n",
    "3. Add audio quality validation\n",
    "4. Deploy to production frontend\n",
    "\"\"\"\n",
    "    \n",
    "    print(summary)\n",
    "    return frontend_data\n",
    "\n",
    "frontend_data = prepare_frontend_data(episodes_data, episode_scripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd932fba",
   "metadata": {},
   "source": [
    "## Pipeline Complete! üéâ\n",
    "\n",
    "The pipeline has successfully transformed the Socratic corpus into structured podcast episodes with audio files and frontend-ready data.\n",
    "\n",
    "### Key Achievements:\n",
    "- ‚úÖ Loaded and processed corpus data\n",
    "- ‚úÖ Generated structured episode scripts\n",
    "- ‚úÖ Created audio files (simulated)\n",
    "- ‚úÖ Updated episodes.json\n",
    "- ‚úÖ Prepared frontend integration\n",
    "\n",
    "### Files Generated:\n",
    "- `src/data/podcast/episodes.json` - Episode metadata\n",
    "- `public/episodes/episode_*.mp3` - Audio files\n",
    "- `src/data/frontend_data.json` - Frontend data\n",
    "\n",
    "The SPA can now load these episodes and play the generated audio content!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
